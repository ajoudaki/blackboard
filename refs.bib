@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\relax \ }Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and et al.},
  journal={arXiv:2201.11903},
  year={2022}
}

@article{nye2021show,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders and Gur-Ari, Guy and et al.},
  journal={arXiv:2112.00114},
  year={2021}
}

@article{wang2022selfconsistency,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and et al.},
  journal={arXiv:2203.11171},
  year={2022}
}

@article{yao2023treeofthoughts,
  title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and et al.},
  journal={arXiv:2305.10601},
  year={2023}
}

@article{chen2022program,
  title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
  journal={arXiv:2211.12588},
  year={2022}
}

@inproceedings{shaw2018relative,
  title={Self-Attention with Relative Position Representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  booktitle={NAACL},
  year={2018}
}

@article{press2021alibi,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah A. and Levy, Mike},
  journal={arXiv:2108.12409},
  year={2021}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv:2104.09864},
  year={2021}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and et al.},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{dehghani2018universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\relax \ }Lukasz},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{geva2021transformerffn,
  title={Transformer Feed-Forward Layers Are Key-Value Memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{saxton2019analysing,
  title={Analysing Mathematical Reasoning Abilities of Neural Models},
  author={Saxton, David and Grefenstette, Edward and others},
  booktitle={ICLR},
  year={2019}
}

@article{zaremba2014execute,
  title={Learning to Execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv:1410.4615},
  year={2014}
}

@article{kaiser2015neuralgpu,
  title={Neural GPUs Learn Algorithms},
  author={Kaiser, {\relax \ }Lukasz and Sutskever, Ilya},
  journal={arXiv:1511.08228},
  year={2015}
}

@article{graves2014ntm,
  title={Neural Turing Machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv:1410.5401},
  year={2014}
}

@article{graves2016dnc,
  title={Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and et al.},
  journal={Nature},
  year={2016}
}


@inproceedings{trask2018nalu,
  title={Neural Arithmetic Logic Units},
  author={Trask, Andrew and Hill, Felix and Reed, Scott and et al.},
  booktitle={NeurIPS},
  year={2018}
}

@article{power2022grokking,
  title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author={Power, Ashton and Burda, Yuri and Edwards, Harri and Klimov, Oleg},
  journal={arXiv:2201.02177},
  year={2022}
}

@article{olah2020zoom,
  title={Zoom In: An Introduction to Circuits},
  author={Olah, Chris and others},
  journal={Distill},
  year={2020}
}

@article{elhage2021transformer,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and et al.},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{olsson2022induction,
  title={In-Context Learning and Induction Heads},
  author={Olsson, Catherine and Rahtz, Matthew and Sharma, Nicholas and et al.},
  journal={Transformer Circuits Thread},
  year={2022}
}

@article{shinn2023reflexion,
  title={Reflexion: Language Agents with Verbal Reinforcement Learning},
  author={Shinn, Noah and Cassano, Federico and Labash, Mohamed and et al.},
  journal={arXiv:2303.11366},
  year={2023}
}

@article{yao2023react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and et al.},
  journal={arXiv:2210.03629},
  year={2023}
}
